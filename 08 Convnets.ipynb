{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "\n",
    ">*You tested above different models with the set of high-level features extracted from a pretrained neural network. However, can you get similar results by (re)training a ConvNet from the pixels?*\n",
    ">* *What accuracy can you achieve?*\n",
    ">* *Can you get good results? - If not, why?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/exts-ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/exts-ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/exts-ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/exts-ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/exts-ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/exts-ml/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Import the packages needed \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to start by loading the images (pixels) and labels from all the sets (train, validation and test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels from npz files\n",
    "with np.load('train.npz', allow_pickle=False) as npz_file:\n",
    "    X_train=npz_file['images']\n",
    "    y_train=npz_file['labels']\n",
    "\n",
    "with np.load('valid.npz', allow_pickle=False) as npz_file:\n",
    "    X_valid=npz_file['images']\n",
    "    y_valid=npz_file['labels']\n",
    "\n",
    "with np.load('test.npz', allow_pickle=False) as npz_file:\n",
    "    X_test=npz_file['images']\n",
    "    y_test=npz_file['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to implement a convolutional neural network with keras api. Let's start by creating the model and adding the layers. The input dimension corresponds to the shape of the pixels for one image. I am also going to print a summary of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 110, 110, 64)      4864      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 55, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 53, 53, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 43264)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 259590    \n",
      "=================================================================\n",
      "Total params: 301,382\n",
      "Trainable params: 301,382\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Convolutional Network\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(filters=64, kernel_size=5, strides=2,\n",
    "                              activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,\n",
    "                              activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(units=6, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to define the loss function, the optimizer and the metrics that I want to monitor with the compile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am also going to add an earlystopping function that will monitor the loss value on the validation set and stop the training if this loss value does not improve anymore for more than 6 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End training when accuracy stops improving\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to fit the network using the train set and setting the validation data to the validation set in order to see the results on the validation set. I do not need to suffle as this was done when I saved the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 281 samples, validate on 139 samples\n",
      "Epoch 1/100\n",
      "281/281 [==============================] - 9s 34ms/step - loss: 1.8438 - acc: 0.1993 - val_loss: 1.7321 - val_acc: 0.2446\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 8s 27ms/step - loss: 1.6558 - acc: 0.3986 - val_loss: 1.6244 - val_acc: 0.4245\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 8s 27ms/step - loss: 1.3708 - acc: 0.6050 - val_loss: 1.5044 - val_acc: 0.3957\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 8s 27ms/step - loss: 0.9941 - acc: 0.7011 - val_loss: 1.4416 - val_acc: 0.4604\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 8s 27ms/step - loss: 0.6548 - acc: 0.8114 - val_loss: 1.4793 - val_acc: 0.4676\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 8s 27ms/step - loss: 0.3734 - acc: 0.8932 - val_loss: 1.6493 - val_acc: 0.4604\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 8s 29ms/step - loss: 0.2093 - acc: 0.9466 - val_loss: 2.1823 - val_acc: 0.3165\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 8s 30ms/step - loss: 0.1382 - acc: 0.9751 - val_loss: 1.8012 - val_acc: 0.4317\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 8s 27ms/step - loss: 0.0533 - acc: 0.9964 - val_loss: 2.1357 - val_acc: 0.4460\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 8s 27ms/step - loss: 0.0191 - acc: 1.0000 - val_loss: 2.2493 - val_acc: 0.4460\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "  x=X_train, y=y_train, validation_data=(X_valid,y_valid),\n",
    "    batch_size=32, epochs=100, shuffle=False, callbacks=[early_stopping] # No need for shuffling and this was done before training samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to compute the test accuracy and save it in an csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 1s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4499999980131785"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "results=pd.DataFrame({\n",
    "        'model': ['cnn'],\n",
    "        'test_accuracy': '{:.3f}'.format(test_acc)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to csv file with results\n",
    "pd.read_csv('results.csv').append(results).to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of training a ConvNet from the pixels are not as good as using the high-level features extracted from a pretrained neural network. This was expected as it is easier for the models to predict when the high-level features are already identified. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
